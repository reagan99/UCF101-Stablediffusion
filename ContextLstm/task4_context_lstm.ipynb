{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:47:22.971407Z",
     "iopub.status.busy": "2022-12-14T20:47:22.970836Z",
     "iopub.status.idle": "2022-12-14T20:47:25.220321Z",
     "shell.execute_reply": "2022-12-14T20:47:25.219536Z"
    },
    "id": "9RYQIJ9C6BVH"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import remotezip as rz\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # 현재 프로그램이 필요에 따라 점진적으로 메모리를 할당하도록 설정\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        # 메모리 증가를 설정하는 중 예외 발생\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:47:25.225152Z",
     "iopub.status.busy": "2022-12-14T20:47:25.224476Z",
     "iopub.status.idle": "2022-12-14T20:47:25.228444Z",
     "shell.execute_reply": "2022-12-14T20:47:25.227619Z"
    },
    "id": "gVIgj-jIA8U8"
   },
   "outputs": [],
   "source": [
    "URL = 'https://storage.googleapis.com/thumos14_files/UCF101_videos.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:47:25.232015Z",
     "iopub.status.busy": "2022-12-14T20:47:25.231525Z",
     "iopub.status.idle": "2022-12-14T20:47:25.235820Z",
     "shell.execute_reply": "2022-12-14T20:47:25.235043Z"
    },
    "id": "lY-x7TaZlK6O"
   },
   "outputs": [],
   "source": [
    "def list_files_from_zip_url(zip_url):\n",
    "\n",
    "  files = []\n",
    "  with rz.RemoteZip(zip_url) as zip:\n",
    "    for zip_info in zip.infolist():\n",
    "      files.append(zip_info.filename)\n",
    "  return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:47:25.239008Z",
     "iopub.status.busy": "2022-12-14T20:47:25.238542Z",
     "iopub.status.idle": "2022-12-14T20:47:25.860604Z",
     "shell.execute_reply": "2022-12-14T20:47:25.859694Z"
    },
    "id": "lYErXAdUr-rk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UCF101/v_ApplyEyeMakeup_g01_c01.avi',\n",
       " 'UCF101/v_ApplyEyeMakeup_g01_c02.avi',\n",
       " 'UCF101/v_ApplyEyeMakeup_g01_c03.avi',\n",
       " 'UCF101/v_ApplyEyeMakeup_g01_c04.avi',\n",
       " 'UCF101/v_ApplyEyeMakeup_g01_c05.avi',\n",
       " 'UCF101/v_ApplyEyeMakeup_g01_c06.avi',\n",
       " 'UCF101/v_ApplyEyeMakeup_g02_c01.avi',\n",
       " 'UCF101/v_ApplyEyeMakeup_g02_c02.avi',\n",
       " 'UCF101/v_ApplyEyeMakeup_g02_c03.avi',\n",
       " 'UCF101/v_ApplyEyeMakeup_g02_c04.avi']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = list_files_from_zip_url(URL)\n",
    "files = [f for f in files if f.endswith('.avi')]\n",
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:47:25.865040Z",
     "iopub.status.busy": "2022-12-14T20:47:25.864767Z",
     "iopub.status.idle": "2022-12-14T20:47:25.868502Z",
     "shell.execute_reply": "2022-12-14T20:47:25.867728Z"
    },
    "id": "yyyivOX0sO19"
   },
   "outputs": [],
   "source": [
    "def get_class(fname):\n",
    "\n",
    "  return fname.split('_')[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:47:25.871898Z",
     "iopub.status.busy": "2022-12-14T20:47:25.871416Z",
     "iopub.status.idle": "2022-12-14T20:47:25.875525Z",
     "shell.execute_reply": "2022-12-14T20:47:25.874749Z"
    },
    "id": "1qnH0xKzlyw_"
   },
   "outputs": [],
   "source": [
    "def get_files_per_class(files):\n",
    "\n",
    "  files_for_class = collections.defaultdict(list)\n",
    "  for fname in files:\n",
    "    class_name = get_class(fname)\n",
    "    files_for_class[class_name].append(fname)\n",
    "  return files_for_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:47:25.879171Z",
     "iopub.status.busy": "2022-12-14T20:47:25.878665Z",
     "iopub.status.idle": "2022-12-14T20:47:25.882102Z",
     "shell.execute_reply": "2022-12-14T20:47:25.881133Z"
    },
    "id": "qPdURg74uUTk"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 101\n",
    "FILES_PER_CLASS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:47:25.885609Z",
     "iopub.status.busy": "2022-12-14T20:47:25.885129Z",
     "iopub.status.idle": "2022-12-14T20:47:25.893301Z",
     "shell.execute_reply": "2022-12-14T20:47:25.892427Z"
    },
    "id": "GUs0xtXsr9i3"
   },
   "outputs": [],
   "source": [
    "files_for_class = get_files_per_class(files)\n",
    "classes = list(files_for_class.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:47:25.896529Z",
     "iopub.status.busy": "2022-12-14T20:47:25.896284Z",
     "iopub.status.idle": "2022-12-14T20:47:25.900642Z",
     "shell.execute_reply": "2022-12-14T20:47:25.899835Z"
    },
    "id": "-YqFARvqwon9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 101\n",
      "Num videos for class[0]: 145\n"
     ]
    }
   ],
   "source": [
    "print('Num classes:', len(classes))\n",
    "print('Num videos for class[0]:', len(files_for_class[classes[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:47:25.904365Z",
     "iopub.status.busy": "2022-12-14T20:47:25.903852Z",
     "iopub.status.idle": "2022-12-14T20:47:25.908061Z",
     "shell.execute_reply": "2022-12-14T20:47:25.907207Z"
    },
    "id": "O3jek4QimIj-"
   },
   "outputs": [],
   "source": [
    "def select_subset_of_classes(files_for_class, classes, files_per_class):\n",
    "\n",
    "  files_subset = dict()\n",
    "\n",
    "  for class_name in classes:\n",
    "    class_files = files_for_class[class_name]\n",
    "    files_subset[class_name] = class_files[:files_per_class]\n",
    "\n",
    "  return files_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:47:25.911410Z",
     "iopub.status.busy": "2022-12-14T20:47:25.910913Z",
     "iopub.status.idle": "2022-12-14T20:47:25.916095Z",
     "shell.execute_reply": "2022-12-14T20:47:25.915302Z"
    },
    "id": "5cjcz6Gpcb-W"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ApplyEyeMakeup',\n",
       " 'ApplyLipstick',\n",
       " 'Archery',\n",
       " 'BabyCrawling',\n",
       " 'BalanceBeam',\n",
       " 'BandMarching',\n",
       " 'BaseballPitch',\n",
       " 'BasketballDunk',\n",
       " 'Basketball',\n",
       " 'BenchPress',\n",
       " 'Biking',\n",
       " 'Billiards',\n",
       " 'BlowDryHair',\n",
       " 'BlowingCandles',\n",
       " 'BodyWeightSquats',\n",
       " 'Bowling',\n",
       " 'BoxingPunchingBag',\n",
       " 'BoxingSpeedBag',\n",
       " 'BreastStroke',\n",
       " 'BrushingTeeth',\n",
       " 'CleanAndJerk',\n",
       " 'CliffDiving',\n",
       " 'CricketBowling',\n",
       " 'CricketShot',\n",
       " 'CuttingInKitchen',\n",
       " 'Diving',\n",
       " 'Drumming',\n",
       " 'Fencing',\n",
       " 'FieldHockeyPenalty',\n",
       " 'FloorGymnastics',\n",
       " 'FrisbeeCatch',\n",
       " 'FrontCrawl',\n",
       " 'GolfSwing',\n",
       " 'Haircut',\n",
       " 'Hammering',\n",
       " 'HammerThrow',\n",
       " 'HandstandPushups',\n",
       " 'HandstandWalking',\n",
       " 'HeadMassage',\n",
       " 'HighJump',\n",
       " 'HorseRace',\n",
       " 'HorseRiding',\n",
       " 'HulaHoop',\n",
       " 'IceDancing',\n",
       " 'JavelinThrow',\n",
       " 'JugglingBalls',\n",
       " 'JumpingJack',\n",
       " 'JumpRope',\n",
       " 'Kayaking',\n",
       " 'Knitting',\n",
       " 'LongJump',\n",
       " 'Lunges',\n",
       " 'MilitaryParade',\n",
       " 'Mixing',\n",
       " 'MoppingFloor',\n",
       " 'Nunchucks',\n",
       " 'ParallelBars',\n",
       " 'PizzaTossing',\n",
       " 'PlayingCello',\n",
       " 'PlayingDaf',\n",
       " 'PlayingDhol',\n",
       " 'PlayingFlute',\n",
       " 'PlayingGuitar',\n",
       " 'PlayingPiano',\n",
       " 'PlayingSitar',\n",
       " 'PlayingTabla',\n",
       " 'PlayingViolin',\n",
       " 'PoleVault',\n",
       " 'PommelHorse',\n",
       " 'PullUps',\n",
       " 'Punch',\n",
       " 'PushUps',\n",
       " 'Rafting',\n",
       " 'RockClimbingIndoor',\n",
       " 'RopeClimbing',\n",
       " 'Rowing',\n",
       " 'SalsaSpin',\n",
       " 'ShavingBeard',\n",
       " 'Shotput',\n",
       " 'SkateBoarding',\n",
       " 'Skiing',\n",
       " 'Skijet',\n",
       " 'SkyDiving',\n",
       " 'SoccerJuggling',\n",
       " 'SoccerPenalty',\n",
       " 'StillRings',\n",
       " 'SumoWrestling',\n",
       " 'Surfing',\n",
       " 'Swing',\n",
       " 'TableTennisShot',\n",
       " 'TaiChi',\n",
       " 'TennisSwing',\n",
       " 'ThrowDiscus',\n",
       " 'TrampolineJumping',\n",
       " 'Typing',\n",
       " 'UnevenBars',\n",
       " 'VolleyballSpiking',\n",
       " 'WalkingWithDog',\n",
       " 'WallPushups',\n",
       " 'WritingOnBoard',\n",
       " 'YoYo']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_subset = select_subset_of_classes(files_for_class, classes[:NUM_CLASSES], FILES_PER_CLASS)\n",
    "list(files_subset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:47:25.919941Z",
     "iopub.status.busy": "2022-12-14T20:47:25.919389Z",
     "iopub.status.idle": "2022-12-14T20:47:25.924979Z",
     "shell.execute_reply": "2022-12-14T20:47:25.924332Z"
    },
    "id": "AH9sWS_6nRz3"
   },
   "outputs": [],
   "source": [
    "def download_from_zip(zip_url, to_dir, file_names):\n",
    "\n",
    "  with rz.RemoteZip(zip_url) as zip:\n",
    "    for fn in tqdm.tqdm(file_names):\n",
    "      class_name = get_class(fn)\n",
    "      zip.extract(fn, str(to_dir / class_name))\n",
    "      unzipped_file = to_dir / class_name / fn\n",
    "\n",
    "      fn = pathlib.Path(fn).parts[-1]\n",
    "      output_file = to_dir / class_name / fn\n",
    "      unzipped_file.rename(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:47:25.928501Z",
     "iopub.status.busy": "2022-12-14T20:47:25.928047Z",
     "iopub.status.idle": "2022-12-14T20:47:25.932129Z",
     "shell.execute_reply": "2022-12-14T20:47:25.931535Z"
    },
    "id": "6ARYc-WLqqNF"
   },
   "outputs": [],
   "source": [
    "def split_class_lists(files_for_class, count):\n",
    "\n",
    "  split_files = []\n",
    "  remainder = {}\n",
    "  for cls in files_for_class:\n",
    "    split_files.extend(files_for_class[cls][:count])\n",
    "    remainder[cls] = files_for_class[cls][count:]\n",
    "  return split_files, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:47:25.935475Z",
     "iopub.status.busy": "2022-12-14T20:47:25.934991Z",
     "iopub.status.idle": "2022-12-14T20:47:25.940845Z",
     "shell.execute_reply": "2022-12-14T20:47:25.940188Z"
    },
    "id": "IHH2Y1M06xoz"
   },
   "outputs": [],
   "source": [
    "def download_ufc_101_subset(zip_url, num_classes, splits, download_dir):\n",
    "\n",
    "  files = list_files_from_zip_url(zip_url)\n",
    "  for f in files:\n",
    "    path = os.path.normpath(f)\n",
    "    tokens = path.split(os.sep)\n",
    "    if len(tokens) <= 2:\n",
    "      files.remove(f) # Remove that item from the list if it does not have a filename\n",
    "  \n",
    "  files_for_class = get_files_per_class(files)\n",
    "\n",
    "  classes = list(files_for_class.keys())[:num_classes]\n",
    "\n",
    "  for cls in classes:\n",
    "    random.shuffle(files_for_class[cls])\n",
    "    \n",
    "  # Only use the number of classes you want in the dictionary\n",
    "  files_for_class = {x: files_for_class[x] for x in classes}\n",
    "\n",
    "  dirs = {}\n",
    "  for split_name, split_count in splits.items():\n",
    "    print(split_name, \":\")\n",
    "    split_dir = download_dir / split_name\n",
    "    split_files, files_for_class = split_class_lists(files_for_class, split_count)\n",
    "    download_from_zip(zip_url, split_dir, split_files)\n",
    "    dirs[split_name] = split_dir\n",
    "\n",
    "  return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = pathlib.Path('./UCF101_subset/')\n",
    "subset_paths = download_ufc_101_subset(URL,\n",
    "                                       num_classes = NUM_CLASSES,\n",
    "                                       splits = {\"train\": 30, \"val\": 10, \"test\": 10},\n",
    "                                       download_dir = download_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:48:18.750016Z",
     "iopub.status.busy": "2022-12-14T20:48:18.749723Z",
     "iopub.status.idle": "2022-12-14T20:48:18.757488Z",
     "shell.execute_reply": "2022-12-14T20:48:18.756728Z"
    },
    "id": "zupvOLYP4D4q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos: 5050\n"
     ]
    }
   ],
   "source": [
    "\n",
    "donwload_path = pathlib.Path('./UCF101_subset/')\n",
    "\n",
    "video_count_train = len(list(donwload_path.glob('train/*/*.avi')))\n",
    "video_count_val = len(list(donwload_path.glob('val/*/*.avi')))\n",
    "video_count_test = len(list(donwload_path.glob('test/*/*.avi')))\n",
    "from pathlib import Path\n",
    "\n",
    "subset_paths = {\n",
    "    'train': Path('UCF101_subset/train'),\n",
    "    'val': Path('UCF101_subset/val'),\n",
    "    'test': Path('UCF101_subset/test')\n",
    "}\n",
    "\n",
    "\n",
    "video_total = video_count_train + video_count_val + video_count_test\n",
    "print(f\"Total videos: {video_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos: 5050\n"
     ]
    }
   ],
   "source": [
    "video_count_train = len(list(download_dir.glob('train/*/*.avi')))\n",
    "video_count_val = len(list(download_dir.glob('val/*/*.avi')))\n",
    "video_count_test = len(list(download_dir.glob('test/*/*.avi')))\n",
    "video_total = video_count_train + video_count_val + video_count_test\n",
    "print(f\"Total videos: {video_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:48:18.899255Z",
     "iopub.status.busy": "2022-12-14T20:48:18.898946Z",
     "iopub.status.idle": "2022-12-14T20:48:18.903827Z",
     "shell.execute_reply": "2022-12-14T20:48:18.903004Z"
    },
    "id": "vNBCiV3bMzpD"
   },
   "outputs": [],
   "source": [
    "def format_frames(frame, output_size):\n",
    "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "  return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:48:18.907544Z",
     "iopub.status.busy": "2022-12-14T20:48:18.907068Z",
     "iopub.status.idle": "2022-12-14T20:48:18.913751Z",
     "shell.execute_reply": "2022-12-14T20:48:18.912926Z"
    },
    "id": "9ujLDC9G7JyE"
   },
   "outputs": [],
   "source": [
    "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 2):\n",
    "\n",
    "  # Read each video frame by frame\n",
    "  result = []\n",
    "  src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "  need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "  if need_length > video_length:\n",
    "    start = 0\n",
    "  else:\n",
    "    max_start = video_length - need_length\n",
    "    start = random.randint(0, max_start + 1)\n",
    "\n",
    "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "  ret, frame = src.read()\n",
    "  result.append(format_frames(frame, output_size))\n",
    "\n",
    "  for _ in range(n_frames - 1):\n",
    "    for _ in range(frame_step):\n",
    "      ret, frame = src.read()\n",
    "    if ret:\n",
    "      frame = format_frames(frame, output_size)\n",
    "      result.append(frame)\n",
    "    else:\n",
    "      result.append(np.zeros_like(result[0]))\n",
    "  src.release()\n",
    "  result = np.array(result)[..., [2, 1, 0]]\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_from_video_file(video_path, n_frames, output_size=(224, 224), frame_step=2):\n",
    "\n",
    "\n",
    "    result = []\n",
    "    src = cv2.VideoCapture(str(video_path))\n",
    "\n",
    "    video_length = int(src.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if video_length < n_frames * frame_step:\n",
    "        print(f\"Video length is too short to extract {n_frames} frames. Skipping video: {video_path}\")\n",
    "        return None\n",
    "\n",
    "    start = random.randint(0, video_length - n_frames * frame_step + 1)\n",
    "    src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "\n",
    "    for _ in range(n_frames):\n",
    "        ret, frame = src.read()\n",
    "        if ret:\n",
    "            frame = cv2.resize(frame, output_size)\n",
    "            result.append(frame)\n",
    "        else:\n",
    "            print(f\"Error reading frame. Skipping video: {video_path}\")\n",
    "            return None\n",
    "\n",
    "        # Move to next frame_step\n",
    "        for _ in range(frame_step - 1):\n",
    "            src.read()\n",
    "\n",
    "    src.release()\n",
    "    result = np.array(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:48:25.827249Z",
     "iopub.status.busy": "2022-12-14T20:48:25.826702Z",
     "iopub.status.idle": "2022-12-14T20:48:25.833188Z",
     "shell.execute_reply": "2022-12-14T20:48:25.832420Z"
    },
    "id": "MVmfLTlw7Ues"
   },
   "outputs": [],
   "source": [
    "class FrameGenerator:\n",
    "  def __init__(self, path, n_frames):\n",
    "\n",
    "      self.path = path\n",
    "      self.n_frames = n_frames\n",
    "      self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
    "      self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "\n",
    "  def get_files_and_class_names(self):\n",
    "      video_paths = list(self.path.glob('*/*.avi'))\n",
    "      classes = [p.parent.name for p in video_paths] \n",
    "      return video_paths, classes\n",
    "\n",
    "  def __call__(self):\n",
    "      video_paths, classes = self.get_files_and_class_names()\n",
    "\n",
    "      pairs = list(zip(video_paths, classes))\n",
    "\n",
    "      random.shuffle(pairs)\n",
    "\n",
    "      for path, name in pairs:\n",
    "          video_frames = frames_from_video_file(path, self.n_frames) \n",
    "          label = self.class_ids_for_name[name]  \n",
    "          yield video_frames, np.int32(label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:48:25.941538Z",
     "iopub.status.busy": "2022-12-14T20:48:25.940877Z",
     "iopub.status.idle": "2022-12-14T20:48:25.977972Z",
     "shell.execute_reply": "2022-12-14T20:48:25.977261Z"
    },
    "id": "HM4NboJr7ck4"
   },
   "outputs": [],
   "source": [
    "# Create the training set\n",
    "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
    "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
    "train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], 10),\n",
    "                                          output_signature = output_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:48:25.982215Z",
     "iopub.status.busy": "2022-12-14T20:48:25.981605Z",
     "iopub.status.idle": "2022-12-14T20:48:26.006976Z",
     "shell.execute_reply": "2022-12-14T20:48:26.006314Z"
    },
    "id": "Pi8-WkOkEXw5"
   },
   "outputs": [],
   "source": [
    "# Create the validation set\n",
    "val_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['val'], 10),\n",
    "                                        output_signature = output_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:48:26.010696Z",
     "iopub.status.busy": "2022-12-14T20:48:26.010193Z",
     "iopub.status.idle": "2022-12-14T20:48:26.595126Z",
     "shell.execute_reply": "2022-12-14T20:48:26.594254Z"
    },
    "id": "V6qXc-6i7eyK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set of frames: (10, 224, 224, 3)\n",
      "Shape of training labels: ()\n",
      "Shape of validation set of frames: (10, 224, 224, 3)\n",
      "Shape of validation labels: ()\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the data\n",
    "train_frames, train_labels = next(iter(train_ds))\n",
    "print(f'Shape of training set of frames: {train_frames.shape}')\n",
    "print(f'Shape of training labels: {train_labels.shape}')\n",
    "\n",
    "val_frames, val_labels = next(iter(val_ds))\n",
    "print(f'Shape of validation set of frames: {val_frames.shape}')\n",
    "print(f'Shape of validation labels: {val_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:48:26.598912Z",
     "iopub.status.busy": "2022-12-14T20:48:26.598289Z",
     "iopub.status.idle": "2022-12-14T20:48:26.613212Z",
     "shell.execute_reply": "2022-12-14T20:48:26.612513Z"
    },
    "id": "QSxjFtxAvY3_"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
    "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T20:48:26.617134Z",
     "iopub.status.busy": "2022-12-14T20:48:26.616397Z",
     "iopub.status.idle": "2022-12-14T20:49:11.066267Z",
     "shell.execute_reply": "2022-12-14T20:49:11.065507Z"
    },
    "id": "pp2Qc6XSFmeB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set of frames: (2, 10, 224, 224, 3)\n",
      "Shape of training labels: (2,)\n",
      "Shape of validation set of frames: (2, 10, 224, 224, 3)\n",
      "Shape of validation labels: (2,)\n"
     ]
    }
   ],
   "source": [
    "train_ds = train_ds.batch(2)\n",
    "val_ds = val_ds.batch(2)\n",
    "\n",
    "train_frames, train_labels = next(iter(train_ds))\n",
    "print(f'Shape of training set of frames: {train_frames.shape}')\n",
    "print(f'Shape of training labels: {train_labels.shape}')\n",
    "\n",
    "val_frames, val_labels = next(iter(val_ds))\n",
    "print(f'Shape of validation set of frames: {val_frames.shape}')\n",
    "print(f'Shape of validation labels: {val_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLSTM\n",
      "Encoder\n",
      "LSTM\n",
      "Attention\n",
      "Epoch 1/100\n",
      "1515/1515 [==============================] - 177s 110ms/step - loss: 4.8076 - accuracy: 0.0178 - val_loss: 9.8181 - val_accuracy: 0.0188 - lr: 5.0000e-06\n",
      "Epoch 2/100\n",
      "1515/1515 [==============================] - 165s 109ms/step - loss: 4.4124 - accuracy: 0.0502 - val_loss: 9.0092 - val_accuracy: 0.0356 - lr: 5.0000e-06\n",
      "Epoch 3/100\n",
      "1515/1515 [==============================] - 163s 107ms/step - loss: 4.0993 - accuracy: 0.1112 - val_loss: 8.3708 - val_accuracy: 0.0426 - lr: 5.0000e-06\n",
      "Epoch 4/100\n",
      "1515/1515 [==============================] - 163s 107ms/step - loss: 3.8340 - accuracy: 0.1818 - val_loss: 7.4392 - val_accuracy: 0.0653 - lr: 5.0000e-06\n",
      "Epoch 5/100\n",
      "1515/1515 [==============================] - 163s 107ms/step - loss: 3.6223 - accuracy: 0.2462 - val_loss: 6.2989 - val_accuracy: 0.1584 - lr: 5.0000e-06\n",
      "Epoch 6/100\n",
      "1515/1515 [==============================] - 161s 106ms/step - loss: 3.4391 - accuracy: 0.2997 - val_loss: 5.5495 - val_accuracy: 0.1693 - lr: 5.0000e-06\n",
      "Epoch 7/100\n",
      "1515/1515 [==============================] - 160s 106ms/step - loss: 3.2895 - accuracy: 0.3436 - val_loss: 5.1882 - val_accuracy: 0.2030 - lr: 5.0000e-06\n",
      "Epoch 8/100\n",
      "1515/1515 [==============================] - 160s 106ms/step - loss: 3.1179 - accuracy: 0.3782 - val_loss: 5.1040 - val_accuracy: 0.2228 - lr: 5.0000e-06\n",
      "Epoch 9/100\n",
      "1515/1515 [==============================] - 161s 106ms/step - loss: 2.9770 - accuracy: 0.4244 - val_loss: 5.9236 - val_accuracy: 0.2248 - lr: 5.0000e-06\n",
      "Epoch 10/100\n",
      "1515/1515 [==============================] - 160s 106ms/step - loss: 2.8818 - accuracy: 0.4446 - val_loss: 4.5387 - val_accuracy: 0.3307 - lr: 5.0000e-06\n",
      "Epoch 11/100\n",
      "1515/1515 [==============================] - 161s 106ms/step - loss: 2.7476 - accuracy: 0.4792 - val_loss: 4.6077 - val_accuracy: 0.3040 - lr: 5.0000e-06\n",
      "Epoch 12/100\n",
      "1515/1515 [==============================] - 166s 110ms/step - loss: 2.6701 - accuracy: 0.4848 - val_loss: 4.5307 - val_accuracy: 0.3495 - lr: 5.0000e-06\n",
      "Epoch 13/100\n",
      "1515/1515 [==============================] - 161s 106ms/step - loss: 2.5778 - accuracy: 0.5218 - val_loss: 4.1523 - val_accuracy: 0.3238 - lr: 5.0000e-06\n",
      "Epoch 14/100\n",
      "1515/1515 [==============================] - 160s 106ms/step - loss: 2.4672 - accuracy: 0.5462 - val_loss: 5.0873 - val_accuracy: 0.2921 - lr: 5.0000e-06\n",
      "Epoch 15/100\n",
      "1515/1515 [==============================] - 160s 106ms/step - loss: 2.3952 - accuracy: 0.5614 - val_loss: 4.0140 - val_accuracy: 0.3644 - lr: 5.0000e-06\n",
      "Epoch 16/100\n",
      "1515/1515 [==============================] - 160s 105ms/step - loss: 2.3188 - accuracy: 0.5818 - val_loss: 4.1499 - val_accuracy: 0.3446 - lr: 4.5242e-06\n",
      "Epoch 17/100\n",
      "1515/1515 [==============================] - 162s 107ms/step - loss: 2.2557 - accuracy: 0.6000 - val_loss: 3.5438 - val_accuracy: 0.4327 - lr: 4.5242e-06\n",
      "Epoch 18/100\n",
      "1515/1515 [==============================] - 163s 108ms/step - loss: 2.1793 - accuracy: 0.6086 - val_loss: 3.6188 - val_accuracy: 0.4347 - lr: 4.5242e-06\n",
      "Epoch 19/100\n",
      "1515/1515 [==============================] - 163s 107ms/step - loss: 2.1246 - accuracy: 0.6172 - val_loss: 4.0524 - val_accuracy: 0.3624 - lr: 4.5242e-06\n",
      "Epoch 20/100\n",
      "1515/1515 [==============================] - 163s 108ms/step - loss: 2.0797 - accuracy: 0.6330 - val_loss: 4.1120 - val_accuracy: 0.3891 - lr: 4.5242e-06\n",
      "Epoch 21/100\n",
      "1515/1515 [==============================] - 163s 107ms/step - loss: 2.0305 - accuracy: 0.6327 - val_loss: 3.5345 - val_accuracy: 0.4317 - lr: 4.5242e-06\n",
      "Epoch 22/100\n",
      "1515/1515 [==============================] - 161s 107ms/step - loss: 1.9834 - accuracy: 0.6436 - val_loss: 3.3066 - val_accuracy: 0.4752 - lr: 4.5242e-06\n",
      "Epoch 23/100\n",
      "1515/1515 [==============================] - 167s 110ms/step - loss: 1.9133 - accuracy: 0.6548 - val_loss: 3.4129 - val_accuracy: 0.5139 - lr: 4.5242e-06\n",
      "Epoch 24/100\n",
      "1515/1515 [==============================] - 166s 110ms/step - loss: 1.8599 - accuracy: 0.6611 - val_loss: 3.3292 - val_accuracy: 0.4406 - lr: 4.5242e-06\n",
      "Epoch 25/100\n",
      "1515/1515 [==============================] - 162s 107ms/step - loss: 1.8488 - accuracy: 0.6772 - val_loss: 2.9954 - val_accuracy: 0.4861 - lr: 4.5242e-06\n",
      "Epoch 26/100\n",
      "1515/1515 [==============================] - 166s 109ms/step - loss: 1.8076 - accuracy: 0.6818 - val_loss: 3.6836 - val_accuracy: 0.4436 - lr: 4.5242e-06\n",
      "Epoch 27/100\n",
      "1515/1515 [==============================] - 165s 109ms/step - loss: 1.7453 - accuracy: 0.6898 - val_loss: 3.7407 - val_accuracy: 0.4396 - lr: 4.5242e-06\n",
      "Epoch 28/100\n",
      "1515/1515 [==============================] - 163s 107ms/step - loss: 1.7336 - accuracy: 0.6828 - val_loss: 3.0571 - val_accuracy: 0.5238 - lr: 4.5242e-06\n",
      "Epoch 29/100\n",
      "1515/1515 [==============================] - 161s 106ms/step - loss: 1.6879 - accuracy: 0.7059 - val_loss: 3.0145 - val_accuracy: 0.4931 - lr: 4.5242e-06\n",
      "Epoch 30/100\n",
      "1515/1515 [==============================] - 162s 107ms/step - loss: 1.6111 - accuracy: 0.7145 - val_loss: 3.0566 - val_accuracy: 0.5644 - lr: 4.5242e-06\n",
      "Epoch 31/100\n",
      "1515/1515 [==============================] - 162s 107ms/step - loss: 1.6234 - accuracy: 0.7284 - val_loss: 3.5324 - val_accuracy: 0.5158 - lr: 3.7041e-06\n",
      "Epoch 32/100\n",
      "1515/1515 [==============================] - 161s 107ms/step - loss: 1.5577 - accuracy: 0.7271 - val_loss: 3.1102 - val_accuracy: 0.5208 - lr: 3.7041e-06\n",
      "Epoch 33/100\n",
      "1515/1515 [==============================] - 162s 107ms/step - loss: 1.5509 - accuracy: 0.7343 - val_loss: 3.4165 - val_accuracy: 0.4762 - lr: 3.7041e-06\n",
      "Epoch 34/100\n",
      "1515/1515 [==============================] - 162s 107ms/step - loss: 1.4885 - accuracy: 0.7403 - val_loss: 2.8825 - val_accuracy: 0.5554 - lr: 3.7041e-06\n",
      "Epoch 35/100\n",
      "1515/1515 [==============================] - 164s 108ms/step - loss: 1.4859 - accuracy: 0.7419 - val_loss: 3.2934 - val_accuracy: 0.4901 - lr: 3.7041e-06\n",
      "Epoch 36/100\n",
      "1515/1515 [==============================] - 159s 105ms/step - loss: 1.4503 - accuracy: 0.7485 - val_loss: 2.9240 - val_accuracy: 0.5505 - lr: 3.7041e-06\n",
      "Epoch 37/100\n",
      "1515/1515 [==============================] - 160s 106ms/step - loss: 1.4463 - accuracy: 0.7568 - val_loss: 2.7528 - val_accuracy: 0.5683 - lr: 3.7041e-06\n",
      "Epoch 38/100\n",
      "1515/1515 [==============================] - 160s 105ms/step - loss: 1.3791 - accuracy: 0.7604 - val_loss: 2.6317 - val_accuracy: 0.6119 - lr: 3.7041e-06\n",
      "Epoch 39/100\n",
      "1515/1515 [==============================] - 160s 106ms/step - loss: 1.4130 - accuracy: 0.7561 - val_loss: 3.2704 - val_accuracy: 0.5267 - lr: 3.7041e-06\n",
      "Epoch 40/100\n",
      "1515/1515 [==============================] - 159s 105ms/step - loss: 1.3232 - accuracy: 0.7782 - val_loss: 3.2252 - val_accuracy: 0.5218 - lr: 3.7041e-06\n",
      "Epoch 41/100\n",
      "1515/1515 [==============================] - 159s 105ms/step - loss: 1.3157 - accuracy: 0.7686 - val_loss: 3.7385 - val_accuracy: 0.4812 - lr: 3.7041e-06\n",
      "Epoch 42/100\n",
      "1515/1515 [==============================] - 160s 105ms/step - loss: 1.3273 - accuracy: 0.7700 - val_loss: 3.0961 - val_accuracy: 0.5594 - lr: 3.7041e-06\n",
      "Epoch 43/100\n",
      "1515/1515 [==============================] - 160s 106ms/step - loss: 1.2901 - accuracy: 0.7733 - val_loss: 3.3487 - val_accuracy: 0.5307 - lr: 3.7041e-06\n",
      "Epoch 44/100\n",
      "1515/1515 [==============================] - 159s 105ms/step - loss: 1.2481 - accuracy: 0.7944 - val_loss: 3.2999 - val_accuracy: 0.5673 - lr: 3.7041e-06\n",
      "Epoch 45/100\n",
      "1515/1515 [==============================] - 159s 105ms/step - loss: 1.2427 - accuracy: 0.7908 - val_loss: 2.5254 - val_accuracy: 0.5762 - lr: 3.7041e-06\n",
      "Epoch 46/100\n",
      "1515/1515 [==============================] - 159s 105ms/step - loss: 1.2200 - accuracy: 0.7855 - val_loss: 2.9874 - val_accuracy: 0.5802 - lr: 2.7441e-06\n",
      "Epoch 47/100\n",
      "1515/1515 [==============================] - 159s 105ms/step - loss: 1.2141 - accuracy: 0.7921 - val_loss: 3.0211 - val_accuracy: 0.5535 - lr: 2.7441e-06\n",
      "Epoch 48/100\n",
      "1515/1515 [==============================] - 159s 105ms/step - loss: 1.2118 - accuracy: 0.7894 - val_loss: 2.6970 - val_accuracy: 0.6257 - lr: 2.7441e-06\n",
      "Epoch 49/100\n",
      "1515/1515 [==============================] - 159s 105ms/step - loss: 1.1974 - accuracy: 0.8003 - val_loss: 2.9477 - val_accuracy: 0.5970 - lr: 2.7441e-06\n",
      "Epoch 50/100\n",
      "1515/1515 [==============================] - 160s 105ms/step - loss: 1.1826 - accuracy: 0.8046 - val_loss: 3.4589 - val_accuracy: 0.5713 - lr: 2.7441e-06\n",
      "Epoch 51/100\n",
      "1515/1515 [==============================] - 159s 105ms/step - loss: 1.1742 - accuracy: 0.8026 - val_loss: 2.8022 - val_accuracy: 0.6010 - lr: 2.7441e-06\n",
      "Epoch 52/100\n",
      "1515/1515 [==============================] - 161s 106ms/step - loss: 1.1397 - accuracy: 0.8102 - val_loss: 2.7771 - val_accuracy: 0.5891 - lr: 2.7441e-06\n",
      "Epoch 53/100\n",
      "1515/1515 [==============================] - 163s 108ms/step - loss: 1.1456 - accuracy: 0.8119 - val_loss: 3.5909 - val_accuracy: 0.5455 - lr: 2.7441e-06\n",
      "Epoch 54/100\n",
      "1515/1515 [==============================] - 165s 109ms/step - loss: 1.1024 - accuracy: 0.8205 - val_loss: 3.0216 - val_accuracy: 0.5416 - lr: 2.7441e-06\n",
      "Epoch 55/100\n",
      "1515/1515 [==============================] - 161s 106ms/step - loss: 1.0903 - accuracy: 0.8188 - val_loss: 2.9276 - val_accuracy: 0.5703 - lr: 2.7441e-06\n",
      "Epoch 56/100\n",
      "1515/1515 [==============================] - 159s 105ms/step - loss: 1.0998 - accuracy: 0.8155 - val_loss: 2.6422 - val_accuracy: 0.6119 - lr: 2.7441e-06\n",
      "Epoch 57/100\n",
      "1515/1515 [==============================] - 159s 105ms/step - loss: 1.0765 - accuracy: 0.8195 - val_loss: 4.2717 - val_accuracy: 0.4554 - lr: 2.7441e-06\n",
      "Epoch 58/100\n",
      "1515/1515 [==============================] - 160s 105ms/step - loss: 1.0823 - accuracy: 0.8129 - val_loss: 2.8427 - val_accuracy: 0.6010 - lr: 2.7441e-06\n",
      "Epoch 59/100\n",
      "1515/1515 [==============================] - 162s 107ms/step - loss: 1.0250 - accuracy: 0.8350 - val_loss: 3.3447 - val_accuracy: 0.5832 - lr: 2.7441e-06\n",
      "Epoch 60/100\n",
      "1515/1515 [==============================] - 162s 107ms/step - loss: 1.0754 - accuracy: 0.8178 - val_loss: 3.0706 - val_accuracy: 0.5663 - lr: 2.7441e-06\n",
      "Epoch 61/100\n",
      "1515/1515 [==============================] - 164s 108ms/step - loss: 1.0232 - accuracy: 0.8307 - val_loss: 2.9315 - val_accuracy: 0.6178 - lr: 1.8394e-06\n",
      "Epoch 62/100\n",
      "1515/1515 [==============================] - 163s 108ms/step - loss: 1.0498 - accuracy: 0.8224 - val_loss: 3.2641 - val_accuracy: 0.5426 - lr: 1.8394e-06\n",
      "Epoch 63/100\n",
      "1515/1515 [==============================] - 163s 108ms/step - loss: 1.0130 - accuracy: 0.8343 - val_loss: 2.9814 - val_accuracy: 0.5911 - lr: 1.8394e-06\n",
      "Epoch 64/100\n",
      "1515/1515 [==============================] - 163s 108ms/step - loss: 0.9739 - accuracy: 0.8396 - val_loss: 2.8129 - val_accuracy: 0.6178 - lr: 1.8394e-06\n",
      "Epoch 65/100\n",
      "1515/1515 [==============================] - 161s 107ms/step - loss: 0.9782 - accuracy: 0.8380 - val_loss: 3.0725 - val_accuracy: 0.5653 - lr: 1.8394e-06\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet152\n",
    "import math\n",
    "\n",
    "class Encoder(models.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        print('Encoder')\n",
    "        super(Encoder, self).__init__()\n",
    "        self.resnet = ResNet152(include_top=False, weights='imagenet', input_shape=(224, 224, 3), pooling='avg')\n",
    "        self.resnet.trainable = False\n",
    "        # Wrap the ResNet model with TimeDistributed to handle the frames dimension\n",
    "        self.time_distributed_resnet = layers.TimeDistributed(self.resnet)\n",
    "        self.time_distributed_dense = layers.TimeDistributed(layers.Dense(latent_dim))\n",
    "        self.time_distributed_bn = layers.TimeDistributed(layers.BatchNormalization(momentum=0.01))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.time_distributed_resnet(inputs)\n",
    "        x = self.time_distributed_dense(x)\n",
    "        x = self.time_distributed_bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LSTMModel(models.Model):\n",
    "    def __init__(self, latent_dim, num_layers, hidden_dim, bidirectional=True):\n",
    "        print('LSTM')\n",
    "        super(LSTMModel, self).__init__()\n",
    "        if bidirectional:\n",
    "            # Bidirectional LSTM layer\n",
    "            self.lstm = layers.Bidirectional(layers.LSTM(hidden_dim, return_sequences=True))\n",
    "        else:\n",
    "            # Unidirectional LSTM layer\n",
    "            self.lstm = layers.LSTM(hidden_dim, return_sequences=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.lstm(inputs)\n",
    "\n",
    "\n",
    "class AttentionModule(models.Model):\n",
    "    def __init__(self, latent_dim, hidden_dim, attention_dim):\n",
    "        print('Attention')\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.latent_attention = layers.Dense(attention_dim)\n",
    "        self.hidden_attention = layers.Dense(attention_dim)\n",
    "        self.joint_attention = layers.Dense(1)\n",
    "\n",
    "    \n",
    "    def call(self, latent_repr, hidden_repr):\n",
    "        h_t = hidden_repr[:, -1, :]  \n",
    "        latent_att = self.latent_attention(latent_repr)\n",
    "        hidden_att = self.hidden_attention(h_t)\n",
    "\n",
    "\n",
    "        hidden_att = tf.expand_dims(hidden_att, 1)  \n",
    "\n",
    "        joint_att = self.joint_attention(layers.ReLU()(latent_att + hidden_att))\n",
    "        attention_w = tf.nn.softmax(joint_att, axis=1)\n",
    "        return attention_w\n",
    "\n",
    "\n",
    "class ConvLSTM(models.Model):\n",
    "    def __init__(self, num_classes, latent_dim=512, lstm_layers=1, hidden_dim=1024, bidirectional=True, attention=True):\n",
    "        print('ConvLSTM')\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.lstm = LSTMModel(latent_dim, lstm_layers, hidden_dim, bidirectional)\n",
    "        self.output_layers = models.Sequential([\n",
    "            layers.Dense(hidden_dim if bidirectional else hidden_dim // 2),\n",
    "            layers.BatchNormalization(momentum=0.01),\n",
    "            layers.ReLU(),\n",
    "            layers.Dense(num_classes, activation='softmax'),\n",
    "        ])\n",
    "        self.attention = attention\n",
    "        if attention:\n",
    "            self.attention_module = AttentionModule(latent_dim, hidden_dim, 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        x = self.lstm(x)\n",
    "        if self.attention:\n",
    "            attention_w = self.attention_module(x, x)\n",
    "            x = tf.reduce_sum(x * attention_w, axis=1)\n",
    "        else:\n",
    "            x = x[:, -1, :] \n",
    "        return self.output_layers(x)\n",
    "\n",
    "\n",
    "class ConvClassifier(models.Model):\n",
    "    def __init__(self, num_classes, latent_dim=512):\n",
    "        print('ConvClassifier')\n",
    "        super(ConvClassifier, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.final = models.Sequential([\n",
    "            layers.Dense(latent_dim),\n",
    "            layers.BatchNormalization(momentum=0.01),\n",
    "            layers.ReLU(),\n",
    "            layers.Dense(num_classes\n",
    "            , activation='softmax'),\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        return self.final(x)\n",
    "    \n",
    "# 모델 생성 및 컴파일\n",
    "conv_lstm_model = ConvLSTM(num_classes=101)\n",
    "conv_lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), metrics = ['accuracy'])\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch > 0 and epoch % 15 == 0:\n",
    "        return lr * math.exp(-0.1 * (epoch // 15))\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "# LearningRateScheduler 콜백 생성\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "# 모델 컴파일\n",
    "conv_lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.000005),\n",
    "                        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "history = conv_lstm_model.fit(train_ds, \n",
    "                    epochs=100,\n",
    "                    validation_data=val_ds,\n",
    "                    callbacks=[tf.keras.callbacks.EarlyStopping(patience=20, monitor='val_loss'), lr_scheduler],\n",
    "                    batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set of frames: (10, 224, 224, 3)\n",
      "Shape of validation labels: (10, 224, 224, 3)\n",
      "505/505 [==============================] - 40s 78ms/step - loss: 3.0725 - accuracy: 0.5653\n",
      "Test Loss: 3.072455883026123\n",
      "Test Accuracy: 0.5653465390205383\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test'], 10),\n",
    "                                        output_signature = output_signature)\n",
    "test_frames, test_labels = next(iter(test_ds))\n",
    "print(f'Shape of validation set of frames: {test_frames.shape}')\n",
    "print(f'Shape of validation labels: {test_frames.shape}')\n",
    "test_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
    "test_frames, test_labels = next(iter(test_ds))\n",
    "test_loss, test_accuracy = conv_lstm_model.evaluate(test_ds)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 159). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model_path\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model_path\\assets\n"
     ]
    }
   ],
   "source": [
    "conv_lstm_model.save('my_model_path', save_format='tf')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "video.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
